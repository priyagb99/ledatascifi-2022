{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Cross-validation is a step where we take our training sample and further divide it in many folds, as in the illustration here:\n",
    "\n",
    "```{image} ./img/feature_5_fold_cv.jpg\n",
    ":alt: 5-fold\n",
    ":width: 400px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "As we talked about in the last chapter, cross-validation allows us to test our models outside the training data more often. This trick reduces the likelihood of overfitting and improves generalization: It _should_ improve our model's performance when we apply it outside the training data.\n",
    "\n",
    "```{warning}\n",
    "I say \"_should_\" because the exact manner in which you create the folds matters. \n",
    "```\n",
    "\n",
    "```{margin}\n",
    "Illustration: If you emulate the simple folding method as depicted in the above graphic for stock return data, some folds will end up testing your model on data from _before_ the periods where the model was estimated!\n",
    "```\n",
    "\n",
    "```{tip}\n",
    "\n",
    "**[This page should be your first reference for available CV splitting options.](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators)** \n",
    "\n",
    "- **If your data has groups** (i.e. repeated observations for a given firm), you should use [group-wise cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#group-cv), like `GroupKFold` to make sure no group is in the training and validation partitions of the fold.\n",
    "- **If your data and/or task is time dependent**, like predicting stock returns,  you should use a [time-wise cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#timeseries-cv), like `TimeSeriesSplit` to ensure that the validation partitions are subsequent to the training sample.\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Using CV in practice\n",
    "\n",
    "Like before, let's first load the data. Notice I consolidated the import lines at the top. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url        = 'https://github.com/LeDataSciFi/ledatascifi-2022/blob/main/data/Fannie_Mae_Plus_Data.gzip?raw=true'\n",
    "fannie_mae = pd.read_csv(url,compression='gzip').dropna()\n",
    "y          = fannie_mae.Original_Interest_Rate\n",
    "fannie_mae = (fannie_mae\n",
    "                  .assign(l_credscore = np.log(fannie_mae['Borrower_Credit_Score_at_Origination']),\n",
    "                          l_LTV = np.log(fannie_mae['Original_LTV_(OLTV)']),\n",
    "                         )\n",
    "              .iloc[:,-11:] # limit to these vars for the sake of this example\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, like before, we then split off some of the data into a testing sample. \n",
    "\n",
    "_For the sake of simplicity (laziness?), let's just reuse the `train_test_split` approach from the last page._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0) # this helps us control the randomness so we can reproduce results exactly\n",
    "X_train, X_test, y_train, y_test = train_test_split(fannie_mae, y, random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{dropdown} **An important digression:**\n",
    "\n",
    "Now that we've introduced some of the conceptual issues with how you create folds for CV, let's revisit this `test_train_split` code above. [This page](https://scikit-learn.org/stable/modules/cross_validation.html#using-cross-validation-iterators-to-split-train-and-test) says `train_test_split` uses [ShuffleSplit](https://scikit-learn.org/stable/modules/cross_validation.html#random-permutations-cross-validation-a-k-a-shuffle-split). This method does not divide by time or any group type. \n",
    "\n",
    "```{dropdown} Q: Does this Fannie Mae data need special attention to how we divide it up?\n",
    "\n",
    "A question to ponder, in class perhaps...\n",
    "\n",
    "```\n",
    "\n",
    "If you want to use any other CV iterators to divide up your sample, you can:\n",
    "\n",
    "```python\n",
    "# Just replace \"GroupShuffleSplit\" with your CV of choice,\n",
    "# and update the contents of split() as needed\n",
    "\n",
    "train_idx, test_idx = next(\n",
    "    GroupShuffleSplit(random_state=7).split(X, y, groups)\n",
    ")\n",
    "X_train, X_test, y_train, y_test = X[train_idx], X[train_idx], y[test_idx], y[test_idx]\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our regularly scheduled \"CV in Practice\" programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **STEP 2:** Set up the CV\n",
    "\n",
    "Sk-learn makes cross-validation pretty easy. We use the `cross_validate(\"estimator\",X_train,y_train,cv,scoring,...)` function ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)) which will \n",
    "1. Create folds in X_train and y_train using whatever method you put in the `cv` parameter. For each fold, it will create a smaller \"training partition\" and \"validation partition\" like in the figure at the top of this page. \n",
    "1. For each fold, \n",
    "    1. It will fit your \"estimator\" on the smaller training partition it creates for that fold (as if you ran `estimator.fit(X_trainingpartition,y_trainingpartition)`)._Note: **Your estimator will actually be a \"pipeline\" object** ([covered in detail on the next page](04e_pipelines)) that tells sk-learn to apply a series of steps to the data (preprocessing, etc.) and always ends with a model to estimate._\n",
    "    1. Use that fitted estimator on the validation partition (as if you ran `estimator.predict(X_validationpartition)`). \n",
    "    1. Score those predictions with the function(s) you put in `scoring`.\n",
    "1. Output a dictionary object with performance data for each fold.\n",
    "\n",
    "```{important}\n",
    "So, to use `cross_validate()`, you need to decide on and set up:\n",
    "\n",
    "1. Your preferred folding method (and number of folds)\n",
    "1. Your estimator (or pipeline ending in an estimator)\n",
    "1. Your scoring method(s) \n",
    "```\n",
    "\n",
    "It's this simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.01695871, 0.00604463, 0.00346589, 0.0063622 , 0.00415468]),\n",
       " 'score_time': array([0.00298142, 0.0017314 , 0.00103474, 0.0025425 , 0.        ]),\n",
       " 'test_score': array([0.90789446, 0.89926394, 0.900032  , 0.90479828, 0.90327986])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_validate\n",
    "\n",
    "cv    = KFold(5)                       # set up fold method\n",
    "ridge = Ridge(alpha=1.0)               # set up model/estimator\n",
    "cross_validate(ridge,\n",
    "               X_train,y_train, cv=cv, \n",
    "               scoring='r2')           # tell it the scoring method here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note} \n",
    "Wow, that was easy! Just 3 lines of code (and an import).\n",
    "```\n",
    "\n",
    "And we can output test score statistics like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9030537085469961\n",
      "0.003162930786979486\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(ridge, X_train, y_train, cv=cv, scoring=\"r2\")\n",
    "print(scores[\"test_score\"].mean())  # scores is just a dictionary\n",
    "print(scores[\"test_score\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step: Pipelines\n",
    "\n",
    "The model above \n",
    "- Only uses a few continuous variables: what if we want to include other variable types (like categorical)?\n",
    "- Uses the variables as given: ML algorithms often need you to transform your variables\n",
    "- Doesn't deal with any data problems (e.g. missing values or outliers)\n",
    "- Doesn't create any interaction terms or polynomial transformations\n",
    "- Uses every variable I give it: But if your input data had 400 variables, you'd be in danger of overfitting!\n",
    "\n",
    "At this point, you are capable of solving all of these problems. (For example, you could clean the data in pandas.)\n",
    "\n",
    "But for our models to be robust to evil monsters like \"data leakage\", we need the fixes to be done within pipelines. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
